

\documentclass[DM,authoryear,toc]{lsstdoc}
% lsstdoc documentation: https://lsst-texmf.lsst.io/lsstdoc.html

% Package imports go here.

% Local commands go here.

% To add a short-form title:
% \title[Short title]{Title}
\title{S17B HSC PDR1 Reprocessing Report}

% Optional subtitle
% \setDocSubtitle{A subtitle}

\author{%
Hsin-Fang Chiang, Greg Daues, Samantha Thrush, and the NCSA team
}

\setDocRef{TEST-31}

\date{\today}

% Optional: name of the document's curator
% \setDocCurator{The Curator of this Document}

\setDocAbstract{%
This document captures information about the large scale HSC reprocessing we performed in Cycle S17B.
}

% Change history defined here.
% Order: oldest first.
% Fields: VERSION, DATE, DESCRIPTION, OWNER NAME.
% See LPM-51 for version number policy.
\setDocChangeRecord{%
  \addtohist{1}{YYY-MM-DD}{Unreleased.}{Hsin-Fang Chiang}
}

\begin{document}

% Create the title page.
% Table of contents is added automatically with the "toc" class option.
\maketitle

% ADD CONTENT HERE
\section{Dataset Information}
The input dataset is the HSC Strategic Survey Program (SSP) Public Data Release 1 (PDR1) \citep{2017arXiv170208449A}.
The PDR1 dataset has been transferred to the LSST GPFS storage /datasets by \jira{DM-9683} and the butler repo is available at /datasets/hsc/repo.

It includes 5654 visits in 7 bands: HSC-G, HSC-R, HSC-I, HSC-Y, HSC-Z, NB0816, NB0921. Their visit IDs are visitId-SSPPDR1.txt.  The official release site is at https://hsc-release.mtk.nao.ac.jp/
The survey has three layers and includes 8 fields.
\begin{enumerate}
\item
UDEEP: SSP{\_}UDEEP{\_}SXDS, SSP{\_}UDEEP{\_}COSMOS
\item
DEEP: SSP{\_}DEEP{\_}ELAIS{\_}N1, SSP{\_}DEEP{\_}DEEP2{\_}3, SSP{\_}DEEP{\_}XMM(S){\_}LSS, SSP{\_}DEEP{\_}COSMOS
\item
WIDE: SSP{\_}WIDE, SSP{\_}AEGIS
\end{enumerate}

The number of visits in each field and band is summarized in the following table.

\input{datasetTable}

The tract IDs for each field, obtained
from https://hsc-release.mtk.nao.ac.jp/doc/index.php/database/
is summarized in the following table.

\input{tractTable}

Plots of tracts and patches can be found on  https://hsc-release.mtk.nao.ac.jp/doc/index.php/data/. In S17B, more tracts than listed were processed.

\section{Hardware}
The processing was done using the Verification Cluster.
The Verification Cluster consists of 48 Dell C6320 nodes with 24 physical cores (2 sockets, 12 cores per processor) and 128 GB RAM. As such, the system provides a total of 1152 physical cores.
lsst-dev01 is a system with 24 physical cores, 256 GB RAM, running the latest CentOS 7.x that serves as the front end of the Verification Cluster.

The Verification Cluster runs the Simple Linux Utility for Resource Management (SLURM) cluster management and job scheduling system. lsst-dev01 runs the SLURM controller and serves as the login or head node , enabling LSST DM users to submit SLURM jobs to the Verification Cluster.

lsst-dev01 and the Verification Cluster utilize the General Parallel File System (GPFS) to provided shared-disk across all of the nodes. The GPFS will have spaces for archived datasets and scratch space to support computation/analysis.



\section{Software}

The LSST software stack is used. A shared software stack on the GPFS file systems, suitable for computation on the Verification Cluster, has been provided and is maintained by Science Pipelines and is available under /software/lsstsw.

The stack version of $w{\_}2017{\_}17$, published on 26-Apr-2017, was used.
Besides, the master branch of $meas{\_}mosaic$, $obs{\_}subaru$, and $ctrl{\_}pool$ from 7-May-2017 and built with $w{\_}2017{\_}17$ was used.
This is equivalent to the week 17 tag with \jira{DM-10315}, \jira{DM-10449}, and \jira{DM-10430}.

Unless otherwise noted, the HSC default config in the stack is used, including the task defaults and $obs{\_}subaru$'s overrides.
That implies the PS1 reference catalog $ps1{\_}pv3{\_}3pi{\_}20170110$ in the LSST format (HTM indexed) is used (/datasets/refcats/htm/ps1{\_}pv3{\_}3pi{\_}20170110/).
The calibration dataset is the 20170105 version from Paul Price; the calibration repo is located at /datasets/hsc/calib/20170105 from \jira{DM-9978}.
The externally provided bright object masks (butler type "brightObjectMask") of version "Arcturus" (\jira{DM-10436}) are added to the repo and applied in coaddDriver.assembleCoadd.

\subsection{Pipeline steps and configs}
\begin{enumerate}
\item
makeSkyMap.py
\item
singleFrameDriver.py     Ignore ccd=9 which has bad amps and results not trustworthy even if processCcd passes
\item
mosaic.py
\item
coaddDriver.py   Make config.assembleCoadd.subregionSize small enough so a full stack of images can fit into memory at once; a trade-off between memory and i/o but doesn't matter scientifically, as the pixels are independent.
\item
multiBandDriver.py
forcedPhotCcd.py   Note: it was added late and hence was not run in the RC processing
\end{enumerate}

Operational configurations, such as logging configurations in $ctrl{\_}pool$, different from the tagged stack may be used (e.g. \jira{DM-10430}).

In the full PDR1 reprocessing, everything was run with the same stack version and config. Reproducible failures are noted below, but no reprocessing is done with a newer software version.

This stack version had a known science problem about bad ellipticity residuals as reported in \jira{DM-10482}; the bug fix \jira{DM-10688} was merged to the stack on May 30 and hence was not applied in this reprocessing campaign.

\subsection{Units of independent execution}

These pipelines will be run no smaller than these units:
\begin{enumerate}
\item
makeSkyMap.py  One SkyMap for everything
\item
singleFrameDriver.py  ccd (typically run per visit)
\item
mosaic.py tract x filter, including all visits overlapping that tract in that filter.
\item
coaddDriver.py patch x filter, including all visits overlapping that patch in that filter. (typically run per tract)
\item
multiBandDriver.py  patch, including all filters. (typically run per tract)
\item
forcedPhotCcd.py  ccd
\end{enumerate}
Data of different layers (DEEP/UDEEP/WIDE) are processed separately.

\subsection{Example commands for processing}
\begin{enumerate}
\item
\begin{verbatim}
makeSkyMap.py  makeSkyMap.py /datasets/hsc/repo --rerun private/username/path
\end{verbatim}
\item
\begin{verbatim}
singleFrameDriver.py  singleFrameDriver.py /datasets/hsc/repo
  --rerun private/username/path/sfm --batch-type slurm
  --mpiexec='-bind-to socket' --cores 24 --time 600 --job jobName2
  --id ccd=0..8^10..103 visit=444
\end{verbatim}
\item
\begin{verbatim}
mosaic.py mosaic.py /datasets/hsc/repo --rerun private/username/path/sfm:private/username/path/mosaic
   --numCoresForRead=12 --id ccd=0..8^10..103 visit=444^446^454^456
   tract=9856 --diagnostics --diagDir=/path/to/mosaic/diag/dir/
\end{verbatim}
\item
\begin{verbatim}
coaddDriver.py coaddDriver.py /datasets/hsc/repo --rerun private/username/path/mosaic:private/username/path/coadd
  --batch-type=slurm --mpiexec='-bind-to socket' --job jobName4
  --time 600 --nodes 1 --procs 12 --id tract=9856 filter=HSC-Y
  --selectId ccd=0..8^10..103 visit=444^446^454^456
\end{verbatim}
\item
\begin{verbatim}
multiBandDriver.py  multiBandDriver.py /datasets/hsc/repo --rerun private/username/path/coadd:private/username/path/multiband
  --batch-type=slurm --mpiexec='-bind-to socket' --job jobName5
  --time 5000 --nodes 1 --procs 12 --id tract=9856 filter=HSC-Y^HSC-I
\end{verbatim}
\item
\begin{verbatim}
forcedPhotCcd.py  forcedPhotCcd.py /datasets/hsc/repo --rerun private/username/path/multiband:private/username/path/forced
   -j 12 --id ccd=0..8^10..103 visit=444
\end{verbatim}
\end{enumerate}


\section{Processing Notes and Reproducible Failures}
\subsection{Summary of Outputs}
All processing were done with the same stack setup. Data of the three layers (UDEEP, DEEP, WIDE) were processed separately.
The output repositories are at:
/datasets/hsc/repo/rerun/DM-10404/UDEEP/
/datasets/hsc/repo/rerun/DM-10404/DEEP/
/datasets/hsc/repo/rerun/DM-10404/WIDE/
All logs are at /datasets/hsc/repo/rerun/DM-10404/logs/

While unnecessary, some edge tracts outside of the PDR1 coverage were attempted in the processing this time. Those data outputs are kept in the repos as well. In other words, there are more tracts in the above output repositories than listed in the tract IDs in the table on top of this page; the additional data can be ignored.

\subsection{Reproducible Failures}
In singleFrameDriver/processCcd, there were reproducible failures in 78 CCDs from 74 visits. Their data IDs are:

--id visit=1206 ccd=77 --id visit=6342 ccd=11 --id visit=6478 ccd=99 --id visit=6528 ccd=24 --id visit=6528 ccd=67 --id visit=6542 ccd=96 --id visit=7344 ccd=67 --id visit=7356 ccd=96 --id visit=7372 ccd=29 --id visit=9736 ccd=67 --id visit=9748 ccd=96 --id visit=9838 ccd=101 --id visit=9868 ccd=76 --id visit=11414 ccd=66 --id visit=13166 ccd=20 --id visit=13178 ccd=91 --id visit=13198 ccd=84 --id visit=13288 ccd=84 --id visit=15096 ccd=47 --id visit=15096 ccd=54 --id visit=15206 ccd=100 --id visit=16064 ccd=101 --id visit=17670 ccd=24 --id visit=17672 ccd=24 --id visit=17692 ccd=8 --id visit=17736 ccd=63 --id visit=17738 ccd=69 --id visit=17750 ccd=58 --id visit=19468 ccd=69 --id visit=23680 ccd=77 --id visit=23798 ccd=76 --id visit=24308 ccd=29 --id visit=25894 ccd=68 --id visit=29324 ccd=99 --id visit=29326 ccd=47 --id visit=29936 ccd=66 --id visit=29942 ccd=96 --id visit=29966 ccd=103 --id visit=30004 ccd=95 --id visit=30704 ccd=101 --id visit=32506 ccd=8 --id visit=33862 ccd=8 --id visit=33890 ccd=61 --id visit=33934 ccd=95 --id visit=33964 ccd=101 --id visit=34332 ccd=61 --id visit=34334 ccd=61 --id visit=34412 ccd=78 --id visit=34634 ccd=61 --id visit=34636 ccd=61 --id visit=34928 ccd=61 --id visit=34930 ccd=61 --id visit=34934 ccd=101 --id visit=34936 ccd=50 --id visit=34938 ccd=95 --id visit=35852 ccd=8 --id visit=35862 ccd=61 --id visit=35916 ccd=50 --id visit=35932 ccd=95 --id visit=36640 ccd=68 --id visit=37342 ccd=78 --id visit=37538 ccd=100 --id visit=37590 ccd=85 --id visit=37988 ccd=33 --id visit=38316 ccd=11 --id visit=38328 ccd=91 --id visit=38494 ccd=6 --id visit=38494 ccd=54 --id visit=42454 ccd=24 --id visit=42510 ccd=77 --id visit=42546 ccd=93 --id visit=44060 ccd=31 --id visit=44090 ccd=27 --id visit=44090 ccd=103 --id visit=44094 ccd=101 --id visit=44162 ccd=61 --id visit=46892 ccd=64 --id visit=47004 ccd=101

Out of the 78 failures:
\begin{enumerate}
\item
36 failed with: "Unable to match sources"
\item
13 failed with: "No objects passed our cuts for consideration as psf stars"
\item
7 failed with: "No sources remaining in match list after magnitude limit cuts"
\item
3 failed with: "No input matches"
\item
3 failed with: "Unable to measure aperture correction for required algorithm 'modelfit{\_}CModel{\_}exp': only 1 sources, but require at least 2."
\item
1 failed with: "All matches rejected in iteration 2"
\item
15 failed with: "PSF star selector found [123] candidates"
\end{enumerate}

In multiBandDriver, two patches of WIDE (tract=9934 patch=0,0  and  tract=9938 patch=0,0) failed with AssertionError as reported in \jira{DM-10574}. I excluded the failed patches from the multiBandDriver commands, and then jobs were able to complete and process all other patches.

The multiBandDriver job of WIDE tract=9457 could not finish unless patch=1,8 is excluded. However tract 9457 is actually outside of the PDR1 coverage.
In forcedPhotCcd, fatal errors were seen about the reference of a patch does not exist; therefore some forced{\_}src were not generated. A JIRA ticket has been filed: \jira{DM-10755}.

\subsection{Low-level processing details}

This section includes low-level details that may only be of interest to the operation team.

The first singleFrame job started on May 8, the last multiband job was May 22, and the last forcedPhotCcd job was on Jun 1.  The processing was done using the Verification Cluster and the GPFS space mounted on it. The NCSA team was responsible of shepherding the run and resolving non-pipeline issues, with close communications with and support from the DRP team regarding the science pipelines.  The "ctrl{\_}pool" style drivers were run on the slurm cluster.

The processing tasks/drivers were run as a total of 8792 slurm jobs:
\begin{enumerate}
\item
514 singleFrame slurm jobs
\item
1555 mosaic slurm jobs
\item
1555 coadd slurm jobs
\item
362 multiband slurm jobs
\item
4806 forcedPhotCcd slurm jobs
\end{enumerate}

For single frame processing, every 11 visits (an arbitrary choice) were grouped into one singleFrameDriver.py command, therefore 5654/11 = 514 jobs in total, and submitted each job to one worker node.  Data of the three layers (DEEP, UDEEP, WIDE) were handled completely separately beginning with the mosaic pipeline step.  skymap.findTractPatchList was used to check through each calexp, find out what tract/patch the ccd overlaps, and write into sqlite3. There are 1555 tract x filter combinations for all three layers. For each tract x filter, all overlapping visits and a template (e.g. mosaic{\_}template.sh) were used to make a slurm job file (such as the .sl file as in \url{https://developer.lsst.io/services/verification.html#verification-slurm}). Similarly for coadd making, each tract x filter was a slurm job, but jobs were submitted using coaddDriver.py.  The multiband processing jobs were submitted for each tract, using multiBandDriver.py.   All numbers here included tracts that were not actually necessary (outside the PDR1 coverage).  For forcedPhotCcd, the CmdLineTask command is written into slurm job files for submission, similar to running mosaic. 21 visits (an arbitrary choice) were grouped in each slurm job in the first batch of submissions; the rest had one visit in each slurm job. In this campaign, at most 24 cores were used on one node at a time and sometimes even fewer.   Hsin-Fang Chiang was aware she did not always run things in the optimal way, and they are to be improved in the coming cycles.

In general, when jobs failed, little effort was spent into investigation as long as the reruns were successful. There were a few transient hardware/file system issues. For example, once a known GPFS hiccup failed two jobs; we happened to know because admins happened to notice it and we happened to match up the timing within a few minutes, but issues like that could easily happen without being noticed.  Other examples of other non-science-pipeline failures are as below.

Failures like the "black hole node" phenomenon were seen a few times. Sometimes many jobs were queued in slurm, and next morning all jobs larger than a job ID were found to be failed without any log being written.  The appearance is that Slurm scheduled numerous jobs in succession, one after another, to a faulty node with a GPFS problem, resulting in a set of failed jobs.   Jobs that started running before that failure point were able to continue as normal. Resubmissions of the same failed jobs were also good.   The observation of a succession of jobs all going to the same problematic node and failing over and over again in a short amount of time motivates an examination of the controller configuration, as there may be Slurm settings that would distribute job and avoid the scenario.

There was an instance that seemed to be a butler repo race condition. When running mosaic processing, multiple jobs seemed to be doing IO with repositoryCfg.yaml and failed at File "/software/lsstsw/stack/Linux64/daf{\_}persistence/13.0-8-gba0f85f/python/lsst/daf/persistence/posixStorage.py", line 189, in putRepositoryCfg and then lsst/daf/persistence/safeFileIo.py", line 84, in FileForWriteOnceCompareSame. Multiple files like "repositoryCfg.yamlGXfgIy" were left in the repo, and they are all the same.  Two possible ways to avoid this: (1) always do a pre-run, or (2) do not let jobs write into the same output repos.

Although large time limits were deliberately used in the slurm jobs, several jobs were timed out and cancelled by slurm, mostly multiband jobs. For new runs, Hsin-Fang Chiang chose to start over with a new output repo rather than letting the driver reuse the existing data. Manual butler repo manipulation was needed to clean up bad executions or combine results.

The pipe{\_}drivers submission could take a while (minutes).

For S17B, the pipeline outputs were written to a production scratch space.  The rerun repos were cleaned up and failures were resolved there. Then the repos were transferred to the archived space at /datasets.  For transferring, this script was helpful: https://wiki.ncsa.illinois.edu/display/~wglick/2013/11/01/Parallel+Rsync


\section{Resource Usage}

\subsection{Disk Usage}

The figures show the disk usage in the production scratch space, which was reserved purely for this S17B campaign use. Tests and failed runs wrote to this space as well.  At hour ~275, removal of some older data in this scratch space was performed so the drop should be ignored.
\begin{figure}[htbp]
        \begin{center}
                 \includegraphics[width=0.45\textwidth]{figures/df_focus}
                 \includegraphics[width=0.45\textwidth]{figures/df}
                 \caption{Disk Usage of the production scratch space throughout the S17B reprocessing}
        \end{center}
\end{figure}

The resultant data products are archived in 4 folders at /datasets/hsc/repo/rerun/DM-10404/. In total there are 11594219 files. The large files are typically hundreds of MBs.  The average size is ~14MB. The file size distribution is as the plot below:

\begin{figure}[htbp]
        \begin{center}
                 \includegraphics[width=\textwidth]{figures/file_size_dist}
                 \caption{file size distribution of the output repos}
        \end{center}
\end{figure}

In terms of butler dataset types, the plots below show the distributions for SFM products and others.  All plots are in log scale.

\begin{figure}[htbp]
        \begin{center}
                 \includegraphics[width=0.7\textwidth]{figures/butler_size_sfm}
                 \includegraphics[width=0.7\textwidth]{figures/butler_size_duw}
                 \caption{Size distribution across butler types}
        \end{center}
\end{figure}

\subsection{CPU Usage}

Total CPU = 79246 core-hours  ~471.7 core-weeks
Total User CPU = 76246 core-hours  ~453.8 core-weeks

The core-hours spent at each pipeline step are:
\begin{enumerate}
\item
sfm: 19596.9
\item
mosaic: 943.2
\item
coadd: 5444.9
\item
multiband: 34127.2
\item
forcedPhotCcd: 19133.9
\end{enumerate}

\begin{figure}[htbp]
        \begin{center}
                 \includegraphics[width=0.6\textwidth]{figures/cpuPieChart}
        \end{center}
\end{figure}

The figure below shows the "efficiency", calculated by dividing the total cpu time by wall elapsed time * number of cores, for each pipeline.

\begin{figure}[htbp]
        \begin{center}
                 \includegraphics[width=\textwidth]{figures/efficiency}
        \end{center}
\end{figure}

 general feature of the plots is that the efficiency is observed to be bounded/limited by the fact that with ctrl{\_}pool/mpi the MPI root process is mostly idle and occupies one core.  This correlates with an upper bound for SFM of 23/24 ~0.958 , for coadd processing of 11/12 ~ 0.916, etc.
sfm: Every 11 visits are grouped into one job, and each visit has 103 ccds. Thus, 1133 ccds were processed in a job, divided amongst 24 cores. Each ccd took around 2 minutes in average; in other words, roughly 90 min of wall clock elapsed time and 36 hr of accumulated CPU time per job. Efficiency is uniformly good. SingleFrameDriverTask is a ctrl{\_}pool BatchParallelTask.  The histogram below shows the CPU time of the SFM slurm jobs. The job IDs of the longest running jobs are: 51245, 51320, 51371, 51483, 51496, 51497, 51525, 51533, 51534, 51536, 51546, 51547, 51548, 51549, 51550, 51582, 51587, 51602, 51603
\begin{figure}[htbp]
        \begin{center}
                 \includegraphics[width=\textwidth]{figures/cpu_sfm}
        \end{center}
\end{figure}

mosaic: The unit of processing is each tract x filter on a node for each layer.  Mosaic jobs used 12 cores for reading source catalogs, via Python multiprocessing, but 1 core for other parts of the task; therefore we did not calculate the efficiency as it would be misleading. MosaicTask does not use ctrl{\_}pool.
coadd: coadd jobs are chosen to process a tract on a node. One tract has 9*9=81 patches. CoaddDriverTask is a ctrl{\_}pool  BatchPoolTask. In most cases the patches are processed “12 wide” using ctrl{\_}pool, distributing the work to 12 cores on a node. Using mpi based ctrl{\_}pool in this context leads to one mostly idle MPI root process and 11 workers.  As Verification nodes have 128 GB RAM, this gives on average ~ 11 GB of memory per patch, with the aggregate  able to use the 128 GB.
MultiBandDriver is a ctrl{\_}pool  BatchPoolTask.
Six multiband jobs (9476-mbWIDE9219, 59482-mbWIDE9737,59484-mbWIDE10050, 59485-mbWIDE10188,59486-mbWIDE16003, 59316-mbUDEEP8522) were excluded from this figure; their elapsed times were very short and had very bad efficiencies but they are from tracks outside of the survey coverage.
Some of the forcedPhotCcd jobs run as only one task on one node had very high efficiency but this gave bad throughput.
Below are the histograms of the maximum resident set size and the virtual memory size for mosaic and forcedPhotCcd. Memory Memory monitoring of ctrl{\_}pool driver jobs (singleFrameDriver, coaddDriver, multiBandDriver) was problematic and we do not believe in the numbers collected, so we do not plot them.

\begin{figure}[htbp]
        \begin{center}
                 \includegraphics[width=\textwidth]{figures/maxm_mosaic}
                 \includegraphics[width=\textwidth]{figures/maxm_forcPhotCcd}
        \end{center}
\end{figure}



\subsection{Node Utilization}

The Verification Cluster in its optimal state has 48 compute nodes with 24 physical cores, 256 GB RAM on each node.  For the duration of the S17B reprocessing there was a peak of 45 compute nodes available. The plot does not include failed jobs or test attempts, of which the generated data do not contribute to the final results directly.

\begin{figure}[htbp]
        \begin{center}
                 \includegraphics[width=\textwidth]{figures/nodeUtilization}
                 \caption{hourly node usage \label{fig:node}}
        \end{center}
\end{figure}

\section{Other Relevant JIRA tickets}
\jira{DM-10782}
\jira{DM-10761}
\jira{DM-10624}
\jira{DM-10413}
\jira{DM-11171}


% Include all the relevant bib files.
% https://lsst-texmf.lsst.io/lsstdoc.html#bibliographies
\bibliography{lsst,lsst-dm,refs_ads,refs,books,local}

\end{document}
